<!DOCTYPE html>
<html lang="kk">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Жасанды Интеллект Қызметтерін Пайдаланудағы Қауіпсіздік</title>
    <style>
        body {
            font-family: "Times New Roman", Times, serif;
            font-size: 18px;
            line-height: 1.6; /* Жақсы оқылу үшін */
        }
        h1, h2, h3, h4 {
             font-weight: bold;
             margin-top: 1.2em;
             margin-bottom: 0.6em;
        }
        h1 {
            font-size: 1.8em; /* Тақырып өлшемдері */
            text-align: center; /* Ортаға туралау */
        }
        h2 {
            font-size: 1.5em;
        }
        h3 {
            font-size: 1.3em;
        }
        h4 {
            font-size: 1.1em;
        }
        p {
             margin-bottom: 1em; /* Абзацтар арасындағы бос орын */
             text-align: justify; /* Мәтінді ені бойынша туралау */
        }
        ol, ul {
            margin-bottom: 1em;
            padding-left: 40px; /* Тізімдерді шегіндіру */
        }
        li {
            margin-bottom: 0.5em; /* Тізім элементтері арасындағы бос орын */
            text-align: justify; /* Тізім элементтерін ені бойынша туралау */
        }
        strong {
             font-weight: bold;
        }
    </style>
</head>
<body>

    <h1>ЖАСАНДЫ ИНТЕЛЛЕКТ ҚЫЗМЕТТЕРІН ПАЙДАЛАНУДАҒЫ ҚАУІПСІЗДІК</h1>

    <p>Жасанды интеллект (AI) құралдары CHATGPT 2022 жылдың қарашасында іске қосылғаннан бері пайдаланушылардың назарын кеңінен аударды. Қолдану біздің жұмысымызды немесе өмірімізді жақсартса да, құралдарды қолданатын және пайдаланушыларды зиянды бағдарламаны жүктеуге немесе құпия ақпаратпен бөлісуге алдау үшін қылмыскерлер болуы мүмкін. Сондықтан ықтимал қауіпсіздік мәселелерін елемеуге болмайды.</p>

    <h2>AI құралдарын пайдалану кезіндегі қауіпсіздік кеңестері:</h2>
    <ol>
        <li><strong>Сенімді AI құралын пайдаланыңыз:</strong> AI құралдарына тек олардың ресми арнасы арқылы қол жеткізіңіз. Қолданбаларды тек ресми қолданбалар сайттарынан орнатыңыз.</li>
        <li><strong>Бірегей және күрделі құпия сөзді пайдаланыңыз:</strong> Егер сіз AI құралдарына кіру үшін есептік жазбаны қолдансаңыз, бірегей және күшті құпия сөзді қолданғаныңызға көз жеткізіңіз. Бірнеше есептік жазба үшін бірдей құпия сөзді пайдаланудан аулақ болыңыз және мүмкіндігінше екі факторлы аутентификацияны (2FA) қосыңыз.</li>
        <li><strong>Қауіпсіз желіні пайдаланыңыз:</strong> Жасанды интеллект құралдарына қол жеткізген кезде Жалпыға Ортақ Wi-Fi желілерін немесе басқа қорғалмаған желілерді пайдаланудан аулақ болыңыз. Қауіпсіз Wi-Fi немесе VPN желісін пайдаланыңыз.</li>
        <li><strong>Жеке немесе құпия ақпаратты бөліспеңіз:</strong> AI құралдарымен аты, мекен-жайы, телефоны, құпия сөздері, жеке сәйкестендіру нөмірі немесе әлеуметтік қауіпсіздік нөмірі немесе қаржылық Ақпарат және т.б. сияқты құпия немесе құпия ақпаратты бөліспеңіз.</li>
        <li><strong>Браузер тарихын тазалаңыз:</strong> AI құралдарын қолданғаннан кейін жазбаларыңыздың немесе сөйлесулеріңіздің барлық іздерін жою үшін шолғыш тарихын, кэшті және cookie файлдарын тазалаңыз.</li>
        <li><strong>Жасанды интеллект құралдары шығаратын нәтижелерді тексеру:</strong> жасанды интеллект құралдары шығаратын нәтижелерді тексеру ұсынылады, өйткені жасанды интеллект құралдары ұсынатын нәтижелер жеткілікті дәл болмауы мүмкін. Сондай-ақ, қосымшаны жасаушылар зиянды кодтың жоқтығына көз жеткізу және қауіпсіздіктің бұзылуын болдырмау үшін AI құралдары жасаған немесе тексерген бағдарламалық кодты екі рет тексеруі керек.</li>
        <li><strong>Бағдарламалық жасақтаманы үнемі жаңартып отырыңыз:</strong> киберқылмыскерлер пайдалана алатын осалдықтардың алдын алу үшін әрқашан операциялық жүйені, қолданбаларды және антивирустық бағдарламалық құралды жаңартыңыз, соңғы қауіпсіздік патчтарын орнатыңыз.</li>
        <li><strong>Фишинг әрекеттерінен сақ болыңыз:</strong> Киберқылмыскерлер басқалардың кейпіне ену және жеке ақпаратты ұрлау үшін жасанды интеллект құралдарын қолдануға тырысуы мүмкін. Әрқашан заңды AI құралдарымен өзара әрекеттесіп жатқаныңызды тексеріңіз және күдікті сілтемелерді басудан немесе тіркемелерді жүктеуден аулақ болыңыз.</li>
    </ol>

    <h2>Жасанды интеллект арқылы таратылатын дезинформациямен күресу</h2>
    <p>Дезинформация – бұл қасақана жасалған, алдауға бағытталған және зиян келтіру мақсатында таратылатын жалған немесе жаңылыстыратын ақпарат. Бұл жай ғана қате ақпараттан (мизинформация) өзгеше, өйткені оның негізінде зиян келтіру ниеті жатыр. Жасанды интеллект (ЖИ) контекстінде бұл ұғым озық алгоритмдер мен машиналық оқыту әдістерін пайдалана отырып, осындай жалған ақпаратты жасау және тарату процесін білдіреді. ЖИ дезинформацияны жасау мен таратуды автоматтандырып, оның ауқымын кеңейтеді және дәстүрлі әдістермен салыстырғанда оны анықтауды айтарлықтай қиындатады.</p>
    <p>ЖИ тудырған дезинформацияның негізгі қаупі оның бұрын-соңды болмаған көлемінде, таралу жылдамдығында және шынайылық деңгейінде жатыр. Бұрын жалған ақпаратты жасау және тарату белгілі бір ресурстарды, уақытты және арнайы дағдыларды қажет ететін болса, қазіргі ЖИ құралдары, мысалы, ChatGPT сияқты үлкен тілдік модельдер (LLM), бағдарламалау дағдылары жоқ адамдарға да аз шығынмен мыңдаған, тіпті миллиондаған жалған хабарлама нұсқаларын лезде жасауға мүмкіндік береді. Сол сияқты, дипфейктерді (deepfakes) – фото, видео және аудио манипуляцияларды жасау бұрын техникалық білім мен қымбат жабдықты талап етсе, қазір тегін немесе арзан онлайн қызметтер арқылы қолжетімді бола бастады. Дезинформация жасау процесінің бұлайша "демократизациялануы" зиянкестердің санын арттырып, олардың мүмкіндіктерін кеңейтеді, бұл өз кезегінде ақпараттық кеңістіктің ластануын күшейтеді.</p>
    <p>Бұл мәселе тек саяси алаңмен немесе қоғамдық пікірмен шектелмейді. ЖИ тудырған дезинформация академиялық ортаға да елеулі экономикалық және репутациялық қауіп төндіреді. Академиялық институттардың негізгі миссиясы – ақиқатты іздеу және білімді тарату болғандықтан, жалған ақпараттың таралуы олардың іргетасына нұқсан келтіреді. Институттар ЖИ дезинформациясын анықтайтын қымбат құралдарға және қызметкерлер мен студенттерді осы қауіпке қарсы тұруға үйретуге қомақты қаржы бөлуге мәжбүр. Бұл ресурстар оқыту, зерттеу және қоғамға қызмет ету сияқты негізгі академиялық қызметтерден алынады. Сонымен қатар, егер академиялық мекеменің аты жалған немесе бұрмаланған ақпаратпен байланыстырылса, бұл оның беделіне нұқсан келтіріп, қоғамдық сенімді жоғалтуға, студенттерді тартуда қиындықтарға, қаржыландырудың азаюына және стратегиялық серіктестіктердің бұзылуына әкелуі мүмкін. Бұл академиялық қауымдастықтың тиімді жұмыс істеуі және зерттеулердің қоғамға оң әсер етуі үшін қажетті іргелі сенімді бұзады.</p>

    <h2>ЖИ-дің дезинформацияны жасау және таратудағы рөлі</h2>
    <p>Жасанды интеллект дезинформацияны жасаудың және таратудың бірнеше негізгі механизмдерін күшейтеді:</p>
    <p><strong>Дипфейктер (Deepfakes):</strong> Бұл ЖИ, әсіресе терең оқыту (deep learning) және нейрондық желілер арқылы жасалған, адамның бет-әлпетін, денесін немесе дауысын өте шынайы түрде өзгертетін немесе толығымен жасайтын медиа контент. Дипфейктер адамдардың ешқашан айтпаған сөздерін айтқызып, жасамаған іс-әрекеттерін жасағандай етіп көрсете алады. Бұл технология көбінесе Генеративті Жарыспалы Желілерге (Generative Adversarial Networks - GANs) негізделген, мұнда бір желі (генератор) жалған контент жасауға тырысса, екінші желі (дискриминатор) оны шынайыдан ажыратуға тырысады, осылайша генератордың сапасы артады. Дипфейктер саясаткерлерді (мысалы, АҚШ президенті Джо Байденнің дауысымен жасалған жалған телефон қоңырауы , Дональд Трамптың , Молдова президенті Майя Сандудың , Камала Харристің жалған бейнелері), атақты адамдарды (мысалы, Тейлор Свифттің, Том Круздың жалған бейнелері), жаңалықтар жүргізушілерін және басқа да қоғам қайраткерлерін нысанаға алуы мүмкін. Дауысты клондау (Voice cloning) технологиясы да осы санатқа жатады және адамдарды алдау, қаржылық алаяқтық жасау немесе сенімді бұзу үшін қолданылуы мүмкін. Мысалы, басшысының дауысымен қоңырау шалған алаяқтарға ақша аударған компания басшысы немесе ұлының дауысымен көмек сұраған қоңырауға сеніп қалған ата-ана туралы оқиғалар бар.</p>
    <p><strong>Автоматтандырылған контент генерациясы:</strong> Үлкен тілдік модельдер (LLM) сияқты ЖИ жүйелері адам жазғандай көрінетін мәтіндерді – жаңалықтар мақалаларын, әлеуметтік желідегі жазбаларды, электрондық хаттарды, тіпті академиялық жұмыстарды жаппай және жылдам жасай алады. Бұл мүмкіндік жалған жаңалық сайттарын құру, спам тарату, фишингтік шабуылдар жасау және саяси пропаганданы кеңінен тарату үшін белсенді қолданылуда. Мысалы, Венесуэланың мемлекеттік БАҚ-тары жоқ халықаралық арнаның ЖИ арқылы жасалған жүргізушілері бар видеоларды про-үкіметтік хабарламалар тарату үшін пайдаланған. ЖИ сондай-ақ автоматты түрде суреттерді, музыканы және басқа да медиа түрлерін жасай алады. Бұл технологияның қолжетімділігі (көбінесе тегін немесе арзан) кез келген адамға аз күш жұмсап, үлкен көлемде контент жасауға мүмкіндік береді.</p>
    <p><strong>Мақсатты кампаниялар (Targeted Campaigns):</strong> ЖИ үлкен деректер жиындарын (пайдаланушылардың онлайн мінез-құлқы, демографиялық мәліметтер, қызығушылықтар) талдау арқылы дезинформациялық хабарламаларды белгілі бір топтарға немесе тіпті жеке тұлғаларға бағыттап, олардың сенімдеріне, қорқыныштарына немесе осалдықтарына әсер ететіндей етіп бейімдей алады. Бұл "микро-таргетинг" немесе "гипер-персонализация" деп аталады және әлеуметтік медиа платформаларының алгоритмдерімен және автоматтандырылған бот желілерімен үйлескенде өте қауіпті болуы мүмкін. ЖИ осылайша сайлау процестеріне араласу, қоғамдағы әлеуметтік жіктерді тереңдету, институттарға деген сенімді бұзу және тіпті зорлық-зомбылықты өршіту үшін қолданылуы мүмкін.</p>
    <p>ЖИ-дің дезинформацияны таратудағы рөлі оны жасаумен ғана шектелмейді. Көптеген әлеуметтік медиа платформалары контентті сұрыптау және ұсыну үшін ЖИ-ге негізделген алгоритмдерді пайдаланады. Бұл алгоритмдер кейде пайдаланушылардың назарын аударатын, бірақ жалған немесе зиянды контенттің вирустық таралуын байқаусызда тездетуі мүмкін. Сонымен қатар, ЖИ арқылы басқарылатын боттар (автоматтандырылған аккаунттар) белгілі бір хабарламаларды немесе хэштегтерді жасанды түрде күшейтіп, олардың танымалдылығын арттырып, шынайы пікірталас ретінде көрсету үшін қолданылады. Осылайша, ЖИ дезинформацияның жасалуынан бастап, оның кең аудиторияға жетуіне дейінгі бүкіл тізбекті күшейтеді.</p>
    <p>Бұл процестердің тағы бір қауіпті салдары – "өтірікшінің дивиденді" (Liar's Dividend) феномені. ЖИ арқылы жасалған контенттің шынайылығы соншалықты жоғары болғандықтан, адамдар кез келген күмәнді немесе өздеріне ұнамайтын ақпаратты, тіпті ол шындық болса да, "дипфейк" немесе "ЖИ жасаған" деп оңай жоққа шығара алады. Бұл жалпы ақпараттық экожүйеге, БАҚ-қа, сарапшыларға және ресми дереккөздерге деген сенімді одан әрі бұзады. Нәтижесінде, қоғам шындық пен жалғандықты ажыратуда қиналатын "пост-шындық" дәуіріне тереңдей түседі. Бұл дезинформациямен күресу стратегияларын әзірлеуде тек жалғандықты анықтау ғана емес, сонымен қатар шынайы ақпараттың сенімділігін қорғау қажеттілігін де тудырады.</p>

    <h2>ЖИ арқылы жасалған дезинформацияны техникалық анықтау</h2>
    <p>ЖИ арқылы жасалған дезинформацияның таралуына қарсы тұрудың маңызды бағыттарының бірі – осындай жалған контентті техникалық құралдармен анықтау. Бұл әдістер контенттің түріне (бейне, аудио, мәтін, бот белсенділігі) байланысты әртүрлі болады.</p>

    <h3>Дипфейктерді анықтау әдістері</h3>
    <p>Дипфейктерді анықтау – белсенді зерттеу саласы болып табылады және бірнеше негізгі тәсілдерді қамтиды:</p>
    <p><strong>Артефакттарға негізделген әдістер:</strong> Бұл тәсіл ЖИ арқылы жасалған бейнелер мен аудиоларда жиі кездесетін, адам көзіне немесе құлағына байқала бермейтін, бірақ техникалық талдау арқылы анықталатын сәйкессіздіктерді немесе "артефакттарды" іздеуге негізделген. Оларға мыналар жатуы мүмкін: бет қимылдарының (әсіресе көздің жыпылықтауы, еріннің қимылы) табиғи еместігі, тері текстурасының тым тегіс немесе бұлыңғыр болуы, жарық пен көлеңкенің физикалық заңдылықтарға сәйкес келмеуі, фонның бұрмалануы немесе "дірілдеуі", көзілдіріктегі шағылысудың дұрыс болмауы, шаш немесе тіс сияқты ұсақ бөлшектердің анық еместігі, аудиодағы табиғи емес интонация, дауыс тембрінің өзгеруі немесе фондық шудың сәйкессіздігі. Бұл әдістер генерация технологияларының ерте кезеңдерінде тиімді болғанымен, технология жетілген сайын артефакттар азайып, оларды анықтау қиындай түседі.</p>
    <p><strong>Машиналық оқыту және терең оқыту:</strong> Қазіргі заманғы дипфейк детекторларының көпшілігі машиналық оқытуға, әсіресе терең оқытуға негізделген. Бұл модельдер шынайы және жалған медианың үлкен дерекқорларында үйретіледі. Олар контенттің көрінбейтін статистикалық заңдылықтарын, пиксель деңгейіндегі немесе жиілік доменіндегі ерекшеліктерді талдау арқылы жалғандықтың нәзік белгілерін анықтауға үйренеді. Бұл салада Конволюциялық нейрондық желілер (CNNs), Transformer негізіндегі архитектуралар (мысалы, Vision Transformer - ViT), CLIP сияқты іргелі модельдерді бейімдеу және басқа да күрделі желілер қолданылады. Бұл модельдер артефакттарға негізделген әдістерге қарағанда жалпылау қабілеті жоғары болуы мүмкін.</p>
    <p><strong>Мультимодальды анықтау:</strong> Кейбір дипфейктер тек визуалды немесе тек аудио деректерді талдау арқылы анықталмауы мүмкін. Мысалы, видеодағы бет кескіні шынайы көрінгенімен, оның ерін қимылдары аудиодағы дыбыспен сәйкес келмеуі мүмкін (lip-sync inconsistency). Мультимодальды тәсілдер видео, аудио және кейде мәтіндік метадеректерді бірге талдап, осындай сәйкессіздіктерді анықтауға тырысады. Бұл тәсіл күрделі манипуляцияларды анықтауда тиімдірек болуы мүмкін.</p>
    <p><strong>Жаңа тәсілдер:</strong> Генерация технологияларының дамуымен (мысалы, диффузиялық модельдер немесе Neural Radiance Fields - NeRFs) детекторлар да жаңа әдістерді қажет етеді. Зерттеушілер білімді енгізу (Knowledge Injection), қабатты басу және контрастты жоғалту (layer-wise suppression and contrast losses), диффузиялық модельдердің кері процесін талдау, әрекет бірліктеріне негізделген талдау (Action Unit-guided analysis) сияқты жаңа тәсілдерді ұсынуда.</p>

    <h4>Дипфейктерді анықтаудағы қиындықтар:</h4>
    <p>Бұл саладағы басты қиындық – детекторлардың жалпылану қабілетінің төмендігі. Яғни, белгілі бір генерация әдісімен жасалған фейктерді анықтауға үйретілген детектор жаңа, бұрын кездеспеген әдіспен жасалған фейктерді анықтауда тиімсіз болуы мүмкін. Сонымен қатар, шынайы өмірдегі деректердің ("in-the-wild") күрделілігі – бейне сапасының төмендігі, сығылу артефакттары, әртүрлі жарықтандыру жағдайлары – детекторлардың жұмысын қиындатады. Пост-өңдеу техникалары, мысалы, видеоның сапасын жақсарту (super-resolution), детекторлардың тиімділігін айтарлықтай төмендетуі мүмкін. Сондай-ақ, зиянкестер детекторларды алдау үшін арнайы қарсылас шабуылдар (adversarial attacks) жасай алады. Нәтижесінде, академиялық зерттеулерде жоғары дәлдік көрсететін көптеген детекторлар шынайы өмірде қолданғанда әлдеқайда нашар нәтиже көрсетуі мүмкін.</p>
    <p>Бұл жағдай дипфейктерді анықтаудың үздіксіз "мысық пен тышқан" ойыны екенін көрсетеді. Генерация технологиялары жетілген сайын, ескі детекторлар тиімсіз болып қалады. Сондықтан, статикалық шешімдер жеткіліксіз; үнемі жаңартылып, жаңа қауіптерге бейімделе алатын динамикалық жүйелер қажет. Сонымен қатар, зертханалық жағдайдағы жоғары нәтижелерге алданып қалмау керек. Детекторлардың шынайы өмірдегі тиімділігінің шектеулі болуы мүмкін екендігі саясаткерлер мен платформалардың тек технологиялық шешімдерге ғана сенбей, медиа сауаттылық, реттеу және басқа да стратегияларды қатар дамытуы керектігін көрсетеді.</p>

    <h3>Синтетикалық мәтінді анықтау құралдары</h3>
    <p>ЖИ арқылы жасалған мәтіндерді (мысалы, LLM шығарған) анықтау да маңызды міндет болып табылады, бірақ оның өзіндік қиындықтары бар.</p>
    <h4>Негізгі тәсілдер:</h4>
    <p><strong>Статистикалық және лингвистикалық талдау:</strong> Бұл әдістер адам жазған және машина жазған мәтіндер арасындағы статистикалық немесе стильдік айырмашылықтарды іздейді. Олар мәтіннің күрделілігін (мысалы, оқылу жеңілдігі индекстері), сөздік қорының байлығын немесе шектеулілігін, белгілі бір сөздердің немесе фразалардың қайталану жиілігін, сөйлем ұзындығының вариациясын, тыныс белгілерін қолдану ерекшеліктерін және басқа да лингвистикалық белгілерді талдайды. Кейбір зерттеулер LLM-дердің мәтін генерациялау кезінде белгілі бір ықтималдық үлестірілімдерін ұстанатынын және осы заңдылықтарды анықтауға болатынын көрсетеді.</p>
    <p><strong>Модельге негізделген әдістер:</strong> Бұл тәсілдер мәтінді жіктеу үшін нейрондық желілерді пайдаланады. Көбінесе BERT, RoBERTa, DeBERTa сияқты алдын ала үйретілген тілдік модельдерге негізделген жіктегіштер қолданылады. Басқа бір қызықты тәсіл – RAIDAR деп аталатын әдіс, мұнда күдікті мәтін басқа бір LLM арқылы қайта жазылады (парафразаланады), содан кейін бастапқы және қайта жазылған мәтіндер арасындағы айырмашылықтар талданады. Сондай-ақ, LLM-нің өзінен берілген мәтінді генерациялау ықтималдығын сұрау арқылы (zero-shot detection) оның ЖИ арқылы жасалғанын бағалауға болады.</p>
    <p><strong>Су таңбалау (Watermarking):</strong> Бұл – мәтінді генерациялау процесі кезінде оған байқалмайтын сандық із немесе "су таңбасын" енгізу әдісі. Бұл белгі кейіннен мәтіннің ЖИ арқылы жасалғанын және қай модельмен жасалғанын дәлелдеуге мүмкіндік береді. Су таңбасы сөздерді таңдау ықтималдығына, тыныс белгілерінің қолданылуына немесе басқа да лингвистикалық ерекшеліктерге енгізілуі мүмкін.</p>

    <h4>Синтетикалық мәтінді анықтаудағы қиындықтар:</h4>
    <p>LLM технологияларының үздіксіз дамуы олардың жасайтын мәтіндерін адам жазғаннан ажыратуды барған сайын қиындатуда. Көптеген детекторлар белгілі бір модельге немесе деректер жиынтығына бейімделгендіктен, олардың жаңа модельдерге, басқа тілдерге немесе әртүрлі тақырыптық домендерге жалпылануы төмен болуы мүмкін. Зиянкестер мәтінді парафразалау немесе басқа да қарсылас шабуылдар арқылы детекторларды оңай алдай алады. Сонымен қатар, детекторлардың қате оң нәтиже (false positive) беру қаупі жоғары, яғни адам жазған мәтінді ЖИ жазған деп белгілеуі мүмкін. Бұл әсіресе академиялық ортада немесе журналистикада елеулі салдарға әкелуі мүмкін.</p>
    <p>Жалпы алғанда, қазіргі қолда бар мәтін анықтау құралдарының дәлдігі мен сенімділігі әлі де жеткіліксіз деңгейде. Мәтінді анықтаудың дипфейктерді анықтаудан да күрделі болуының себебі, мәтінде визуалды немесе аудио артефакттар сияқты айқын "іздер" болмайды. Анықтау көбінесе статистикалық немесе стильдік ерекшеліктерге сүйенеді, ал заманауи LLM-дер бұл жағынан адам жазуына өте жақын, тіпті кейде асып түсетіндей деңгейге жетті. LLM-дер жақсарған сайын, олардың туындыларын анықтау да қиындай береді. Су таңбалау техникалық тұрғыдан перспективалы шешім болып көрінгенімен, оның кеңінен енгізілуі барлық негізгі LLM әзірлеушілерінің ынтымақтастығын және ортақ стандартты қабылдауын талап етеді, бұл бәсекелестік жағдайында қиынға соғуы мүмкін. Сонымен қатар, су таңбаларын парафразалау немесе басқа да өңдеу әдістері арқылы жоюға немесе айналып өтуге болады, бұл оның практикалық тиімділігін шектейді.</p>

    <h3>2.3. Автоматтандырылған бот белсенділігін анықтау</h3>
    <p>Әлеуметтік медиада дезинформацияны таратуда автоматтандырылған аккаунттар немесе боттар маңызды рөл атқарады. Оларды анықтау үшін келесі әдістер қолданылады:</p>
    <p><strong>Ерекшеліктерге негізделген әдістер (Feature-based):</strong> Бұл ең кең таралған тәсілдердің бірі. Ол боттар мен адамдардың мінез-құлқындағы айырмашылықтарды көрсететін әртүрлі ерекшеліктерді талдауға негізделген. Бұл ерекшеліктер бірнеше санатқа бөлінеді:</p>
    <ul>
        <li><strong>Аккаунт метадеректері:</strong> Аккаунттың жасы, жазылушылар мен жазылымдар санының арақатынасы, профиль суретінің болуы/болмауы, профиль сипаттамасының толықтығы, верификация статусы.</li>
        <li><strong>Контент ерекшеліктері:</strong> Жазбалардың мазмұнын табиғи тілді өңдеу (NLP) арқылы талдау, қайталанатын немесе шаблонды хабарламалар, хэштегтерді немесе сілтемелерді шамадан тыс пайдалану, жазбалардың тональдылығы (sentiment analysis).</li>
        <li><strong>Уақыттық белсенділік:</strong> Жариялау жиілігінің өте жоғары немесе тым тұрақты болуы (периодтылық), белсенділік уақыты (мысалы, тәулік бойы белсенді болу), басқа аккаунттармен үйлестірілген әрекеттер (мысалы, бір уақытта бірдей контентті тарату).</li>
        <li><strong>Желілік құрылым:</strong> Аккаунттың әлеуметтік графтағы орны, оның жазылушылары мен жазылымдарының сипаты (олар да бот па?), басқа аккаунттармен өзара әрекеттесу үлгілері (лайк, репост, пікір алмасу).</li>
    </ul>
    <p><strong>Графқа негізделген әдістер:</strong> Бұл әдістер жеке аккаунттарды емес, олардың арасындағы байланыстарды талдауға бағытталған. Боттар көбінесе топтасып жұмыс істейтіндіктен (ботнеттер), олардың арасында тығыз байланыстар немесе ерекше граф құрылымдары (мысалы, "жұлдыз" немесе тығыз кластерлер) байқалуы мүмкін. Бұл әдістер үйлестірілген зиянды әрекеттерді анықтауда тиімдірек болуы мүмкін.</p>
    <p><strong>Кластерлеу және қалыптан тыс жағдайларды анықтау:</strong> Бұл тәсілдер пайдаланушылардың мінез-құлқын ұқсастықтары бойынша топтастырып, қалыпты мінез-құлық үлгілерінен айтарлықтай ауытқитын аккаунттарды немесе топтарды "қалыптан тыс" немесе "күдікті" деп белгілейді. Бұл әдістер алдын ала белгіленген деректерді қажет етпеуі мүмкін (unsupervised).</p>
    <p><strong>Адам мен боттың айырмашылықтарын талдау:</strong> Кейбір зерттеулер боттар мен адамдардың коммуникация стиліндегі айырмашылықтарға назар аударады. Мысалы, боттар көбінесе автоматтандыруға оңай лингвистикалық белгілерді (көп хэштег, позитивті немесе негативті терминдерді жиі қолдану) пайдаланса, адамдар диалогты түсінуді және контекстке жауап беруді қажет ететін әрекеттерді (мысалы, пікірлерге жауап беру, талқылауға қатысу) жиірек жасайды. Сондай-ақ, олардың әлеуметтік желідегі өзара әрекеттесу құрылымдарында да айырмашылықтар болуы мүмкін.</p>

    <h4>Боттарды анықтаудағы қиындықтар:</h4>
    <p>Боттар үнемі жетілдіріліп, адам мінез-құлқын жақсырақ имитациялайтын болып келеді, бұл оларды анықтауды қиындатады. Кейбір күрделі боттарды адамдардың өздері де ажырата алмайды. Боттарды анықтауға арналған дерекқорлардың сапасы, толықтығы және әртүрлілігі де маңызды мәселе болып табылады; бір дерекқорда үйретілген модель басқа жағдайда тиімсіз болуы мүмкін. "Бот" ұғымының өзі әртүрлі түсіндіріледі, және барлық автоматтандырылған аккаунттар зиянды емес. Botometer сияқты танымал құралдар болғанымен, олардың нақты жағдайлардағы дәлдігі мен сенімділігіне қатысты күмәндар бар. Ең басты қиындықтардың бірі – боттардың үнемі эволюциясы; детекторлар жаңа буын боттарына қарсы тұру үшін үнемі жаңартылып отыруы керек.</p>
    <p>Боттарды анықтау саласындағы маңызды түйін – бұл тек техникалық мәселе емес, сонымен қатар концептуалды мәселе екендігінде. "Бот" дегеніміз не және оның қандай түрі зиянды? Кейбір автоматтандырылған аккаунттар жаңалықтарды тарату, ауа райын болжау немесе клиенттерге қызмет көрсету сияқты пайдалы функцияларды орындайды. Сондықтан, тек аккаунттың автоматтандырылған екенін анықтау жеткіліксіз. Оның ниетін, мінез-құлқын және экожүйеге әсерін бағалау маңыздырақ болады. Бұл тек техникалық анықтаудан гөрі, күрделірек, контекстке негізделген талдауды қажет етеді.</p>
    <p>Осыған байланысты, боттарды анықтаудағы ең перспективалы бағыттардың бірі – жекелеген аккаунттардың ерекшеліктеріне тым көп сүйенуден гөрі, үйлестірілген әрекеттерді (coordinated activity) және топтық мінез-құлықты анықтауға көшу болып табылады. Жекелеген күрделі боттар адамдардан айнымауы мүмкін, бірақ олардың жаппай, синхронды және үйлестірілген әрекеттері (мысалы, бірнеше аккаунттың бір уақытта бірдей контентті таратуы немесе бір-біріне жазылуы) көбінесе анықтауға болатын із қалдырады. Бұл тәсілдер (group-based detection) боттардың эволюциясына төзімдірек болуы мүмкін және болашақ зерттеулер мен әзірлемелер үшін негізгі бағыт болып саналады.</p>

    <h2>Қорытынды: Негізгі стратегиялар мен қиындықтар</h2>

    <p>Жасанды интеллект арқылы таралатын дезинформациямен күрес – бұл технологиялық, саяси, әлеуметтік және этикалық өлшемдерді қамтитын, үнемі дамып отыратын күрделі мәселе. Бұл есепте талданған негізгі стратегиялар мен қиындықтарды қорытындылай келе, келесі тұжырымдарды жасауға болады:</p>

    <h3>Негізгі стратегиялар:</h3>
    <ol>
        <li><strong>Технологиялық анықтауды дамыту:</strong> Дипфейктерді, синтетикалық мәтінді және боттарды анықтайтын ЖИ негізіндегі құралдарды үнемі жетілдіру қажет. Бұл ретте, детекторлардың жалпылану қабілетін арттыруға, шынайы өмірдегі деректерге бейімдеуге және қарсылас шабуылдарға төзімділігін күшейтуге баса назар аудару керек. Мультимодальды және топтық мінез-құлықты талдауға негізделген тәсілдер перспективалы болып көрінеді.</li>
        <li><strong>ЖИ-ді қарсы әрекет үшін пайдалану:</strong> ЖИ-ді фактчекинг процестерін автоматтандыру және жылдамдату, дереккөздердің сенімділігін бағалау және зиянды желілер мен үйлестірілген кампанияларды анықтау үшін белсенді қолдану керек. Алайда, ЖИ-ді адам сараптамасын толықтыратын құрал ретінде қарастырған жөн.</li>
        <li><strong>Платформалық жауапкершілікті арттыру:</strong> Әлеуметтік медиа платформалары мен іздеу жүйелері ЖИ контентіне қатысты анық, мөлдір және дәйекті саясаттарды енгізуі және орындауы тиіс. Бұл контентті белгілеуді, тиімді модерацияны, алгоритмдік мөлдірлікті және жарнамаға қатысты қатаң ережелерді қамтиды. Алайда, Community Notes сияқты краудсорсингтік модельдердің тиімділігі әлі де күмәнді.</li>
        <li><strong>Реттеушілік базаны жетілдіру:</strong> Мемлекеттер ЖИ тудыратын қауіптерге жауап беретін заңнамалық және реттеушілік шараларды қабылдауы қажет. ЕО ЖИ Актісі сияқты тәуекелге негізделген, мөлдірлік пен есеп берушілікті талап ететін тәсілдер үлгі бола алады. Қазақстанның қолданыстағы заңнамасын ЖИ ерекшеліктерін ескере отырып қайта қарау және "жалған ақпарат" туралы заңның сөз бостандығына қатысты мәселелерін шешу маңызды.</li>
        <li><strong>Медиа сауаттылықты жаппай арттыру:</strong> Қоғамның дезинформацияға қарсы тұрақтылығын қалыптастыру үшін медиа сауаттылықты білім берудің барлық деңгейлеріне енгізу және қоғамдық хабардарлық науқандарын үнемі жүргізу қажет. Бұл сыни ойлауды, ақпаратты тексеру дағдыларын және белгісіздікпен жұмыс істеу қабілетін дамытуды қамтиды.</li>
        <li><strong>Этикалық принциптерді сақтау:</strong> Дезинформациямен күресу шараларының өздері этикалық нормаларға сәйкес болуы тиіс. Бұл анықтау алгоритмдеріндегі біржақтылықты азайтуды, сөз бостандығын негізсіз шектемеуді, анықтау құралдарын теріс пайдаланудан қорғауды және мөлдірлік пен есеп берушілікті қамтамасыз етуді талап етеді.</li>
        <li><strong>Көпжақты ынтымақтастық:</strong> Бұл мәселені тиімді шешу үшін үкіметтер, технологиялық компаниялар, зерттеу институттары, азаматтық қоғам ұйымдары, БАҚ және жеке тұлғалар арасында тығыз ынтымақтастық қажет.</li>
    </ol>
</body>
</html>